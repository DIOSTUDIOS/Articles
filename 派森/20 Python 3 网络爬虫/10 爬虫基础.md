> 人生苦短，我用 Python

网络爬虫是一种按照一定规则自动抓取网络信息的程序或脚本。简单来说，网络爬虫就是根据一定的算发实现编程开发，主要通过 URL 实现数据的抓取和挖掘。

### 法律法规

目前，国家还没有出台相关的法律法规来约束爬虫行为。但是，从以往的案例来看，将爬取的数据用于个人使用或者科研教学，基本不存在问题；如果用于商业盈利，触犯数据原始拥有者的利益，就有可能被处罚并赔偿原告。

#### Robots 协议

Robots 协议（Robots Exclusion Protocol，爬虫协议）全称为**网络爬虫排除标准**，网站通过 Robots 协议告诉搜索引擎哪写页面可以抓取，哪些页面不能抓取。该协议是国际互联网界通行的道德规范，虽然没有被写入法律，但是每一个爬虫都应该遵守这项协议。

#### 爬虫约束

爬虫过于快速或者频繁的访问服务器，会对其产生巨大的压力，网站可能封锁你的 IP，甚至网站的所有者可能会采取进一步的法律行动。因此，需要约束自己爬虫的网络行为，将请求速度限制在一个合理的范围之内。

#### 违法风险

- 利用爬虫技术与黑客技术结合，攻击网站后台，从而窃取后台数据
- 利用爬虫恶意攻击网站，造成网站系统的瘫痪

### 爬虫用途



### 爬虫分类

- 通用型：又称全网爬虫，最典型的就是各大搜索引擎
- 聚焦型：又称主题爬虫，根据需求有选择、有目的的爬取页面数据
- 增量式：指对已下载的网页采取增量式更新，并只爬取新产生的，或者发生变化的内容
- 深层次：大部分内容不能通过静态 URL 获取的、隐藏在搜索表单后的、只有用户提交一些关键词才能获得的网络页面，需要使用此类爬虫获取数据

### 工作流程



### 开发流程

- 需求说明
- 开发计划
- 功能开发
- 功能测试
- 部署交付

### 网页结构

